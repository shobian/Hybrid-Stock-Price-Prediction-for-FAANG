{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: streamlit\n",
      "Version: 1.43.2\n",
      "Summary: A faster way to build and share data apps\n",
      "Home-page: https://streamlit.io\n",
      "Author: Snowflake Inc\n",
      "Author-email: hello@streamlit.io\n",
      "License: Apache License 2.0\n",
      "Location: /Users/shobians./Library/Python/3.9/lib/python/site-packages\n",
      "Requires: altair, blinker, cachetools, click, gitpython, numpy, packaging, pandas, pillow, protobuf, pyarrow, pydeck, requests, tenacity, toml, tornado, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scaler\n",
      "  Downloading scaler-1.9.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting bidict (from scaler)\n",
      "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting cloudpickle (from scaler)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: psutil in /Users/shobians./Library/Python/3.9/lib/python/site-packages (from scaler) (5.9.8)\n",
      "Collecting pycapnp (from scaler)\n",
      "  Downloading pycapnp-2.0.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: pyzmq in /Users/shobians./Library/Python/3.9/lib/python/site-packages (from scaler) (24.0.1)\n",
      "Collecting tblib (from scaler)\n",
      "  Downloading tblib-3.0.0-py3-none-any.whl.metadata (25 kB)\n",
      "Downloading scaler-1.9.1-py3-none-any.whl (133 kB)\n",
      "Downloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading pycapnp-2.0.0-cp39-cp39-macosx_11_0_arm64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tblib-3.0.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: tblib, pycapnp, cloudpickle, bidict, scaler\n",
      "Successfully installed bidict-0.23.1 cloudpickle-3.1.1 pycapnp-2.0.0 scaler-1.9.1 tblib-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-03-17 01:14:49.865 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-17 01:14:49.956 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/shobians./Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-03-17 01:14:49.956 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-17 01:14:49.957 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-17 01:14:49.957 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-17 01:14:49.957 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-17 01:14:49.957 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-17 01:14:49.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-17 01:14:49.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-17 01:14:49.958 Session state does not function when running a script without `streamlit run`\n",
      "2025-03-17 01:14:49.958 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-17 01:14:49.959 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-17 01:14:49.959 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-17 01:14:49.959 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-17 01:14:49.960 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-17 01:14:49.960 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-17 01:14:49.960 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import yfinance as yf\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from newspaper import Article\n",
    "from GoogleNews import GoogleNews\n",
    "from textblob import TextBlob\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "# Define loss function explicitly\n",
    "custom_objects = {\"mse\": MeanSquaredError()}\n",
    "\n",
    "# Load the LSTM model\n",
    "lstm_model_loaded = load_model(\"models/lstm_model.h5\", custom_objects=custom_objects)\n",
    "\n",
    "# Load each machine learning model individually\n",
    "linear_regression_model_loaded = joblib.load(\"models/linear_regression_model.pkl\")\n",
    "optimized_random_forest_model_loaded = joblib.load(\"models/optimized_random_forest_model.pkl\")\n",
    "optimized_xgboost_model_loaded = joblib.load(\"models/optimized_xgboost_model.pkl\")\n",
    "optimized_lightgbm_model_loaded = joblib.load(\"models/optimized_lightgbm_model.pkl\")\n",
    "gradient_boosting_model_loaded = joblib.load(\"models/gradient_boosting_model.pkl\")\n",
    "svr_model_loaded = joblib.load(\"models/svr_model.pkl\")\n",
    "\n",
    "# Load the Prophet model\n",
    "prophet_model_loaded = joblib.load(\"models/prophet_model.pkl\")\n",
    "\n",
    "#     Load Scaler (Assuming RobustScaler was used in training)\n",
    "scaler = joblib.load(\"models/scaler.pkl\")\n",
    "\n",
    "#     Fetch Real-Time Stock Data\n",
    "def get_stock_data(ticker):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    df = stock.history(period=\"7d\")  # Last 60 days\n",
    "    return df\n",
    "\n",
    "#     Fetch Google News Headlines & Perform Sentiment Analysis\n",
    "def get_news_sentiment(ticker):\n",
    "    googlenews = GoogleNews(lang='en', period='3d')\n",
    "    googlenews.search(ticker)\n",
    "    news_list = googlenews.result()\n",
    "    \n",
    "    sentiments = {\"polarity\": [], \"neg\": [], \"neu\": [], \"pos\": []}\n",
    "    \n",
    "    for news in news_list[:10]:  # Limit to 10 articles\n",
    "        try:\n",
    "            article = Article(news[\"link\"])\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            article.nlp()\n",
    "            text = article.text\n",
    "            sentiment = TextBlob(text).sentiment\n",
    "            sentiments[\"polarity\"].append(sentiment.polarity)\n",
    "            sentiments[\"neg\"].append(1 if sentiment.polarity < 0 else 0)\n",
    "            sentiments[\"neu\"].append(1 if sentiment.polarity == 0 else 0)\n",
    "            sentiments[\"pos\"].append(1 if sentiment.polarity > 0 else 0)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(sentiments).mean()\n",
    "\n",
    "#     Predict Stock Price using Loaded Models\n",
    "def predict_stock_price(features):\n",
    "    predictions = {\n",
    "        \"Linear Regression\": linear_regression_model_loaded.predict(features)[0],\n",
    "        \"Random Forest\": optimized_random_forest_model_loaded.predict(features)[0],\n",
    "        \"XGBoost\": optimized_xgboost_model_loaded.predict(features)[0],\n",
    "        \"LightGBM\": optimized_lightgbm_model_loaded.predict(features)[0],\n",
    "        \"SVR\": svr_model_loaded.predict(features)[0],\n",
    "        \"Gradient Boosting\": gradient_boosting_model_loaded.predict(features)[0],\n",
    "        \"Prophet\": prophet_model_loaded.predict(features)[0]\n",
    "    }\n",
    "    \n",
    "    #     LSTM Prediction\n",
    "    lstm_features = np.array(features).reshape(1, 1, -1)\n",
    "    lstm_pred = lstm_model_loaded.predict(lstm_features)[0][0]\n",
    "    predictions[\"LSTM\"] = lstm_pred\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "#     Streamlit Web App\n",
    "st.title(\"    Real-Time Stock Price Prediction\")\n",
    "st.sidebar.header(\"Select FAANG Company\")\n",
    "ticker = st.sidebar.selectbox(\"Choose a stock\", [\"AAPL\", \"GOOGL\", \"META\", \"NFLX\", \"AMZN\"])\n",
    "\n",
    "if st.sidebar.button(\"Predict Stock Price\"):\n",
    "    with st.spinner(\"Fetching data & making predictions...\"):\n",
    "        stock_data = get_stock_data(ticker)\n",
    "        news_sentiment = get_news_sentiment(ticker)\n",
    "\n",
    "        #     Prepare Features for Prediction\n",
    "        latest_price = stock_data[\"Close\"].iloc[-1]\n",
    "        rsi = stock_data[\"Close\"].diff().rolling(window=14).mean().iloc[-1]\n",
    "        macd = stock_data[\"Close\"].ewm(span=12).mean().iloc[-1] - stock_data[\"Close\"].ewm(span=26).mean().iloc[-1]\n",
    "        ma20 = stock_data[\"Close\"].rolling(window=20).mean().iloc[-1]\n",
    "\n",
    "        features = pd.DataFrame([[rsi, macd, ma20, news_sentiment[\"polarity\"], news_sentiment[\"neg\"],\n",
    "                                  news_sentiment[\"neu\"], news_sentiment[\"pos\"]]], \n",
    "                                columns=[\"RSI\", \"MACD\", \"MA20\", \"sentiment_polarity\", \n",
    "                                         \"sentiment_neg\", \"sentiment_neu\", \"sentiment_pos\"])\n",
    "        \n",
    "        # Scale Features\n",
    "        features_scaled = scaler.transform(features)\n",
    "        \n",
    "        # Make Predictions\n",
    "        predictions = predict_stock_price(features_scaled)\n",
    "\n",
    "        # Display Results\n",
    "        st.subheader(f\"  Predicted Stock Prices for {ticker}\")\n",
    "        for model, pred_price in predictions.items():\n",
    "            st.write(f\"  {model}: **${pred_price:.2f}**\")\n",
    "\n",
    "        # Fetch Real Price for Comparison\n",
    "        actual_price = stock_data[\"Close\"].iloc[-1]\n",
    "        st.subheader(f\"    Actual Closing Price: **${actual_price:.2f}**\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
